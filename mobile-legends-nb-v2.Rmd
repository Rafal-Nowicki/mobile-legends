---
title: "Mobile legend analysis"
author: "Rafal Nowicki"
date: "5 04 2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(robotstxt)
library(tidyverse)
library(ggcorrplot)
library(magrittr)
library(knitr)
library(ggcorrplot)
library(see)
library(latex2exp)
library(car)
library(RColorBrewer) 
library(gtrendsR)
library(GGally)
library(plotly)
library(kableExtra)
library(factoextra)
library(pracma)
library(pca3d)
library(Hmisc)
library(ggdendro)
library(dendextend)
library(gridExtra)
```

## Preliminary

Mobile Legends: Bang Bang is a MOBA game (Multiplayer online battle arena) for mobile devices with Android and iOS developed by Shanghai Moonton. The game was originally released in Asia on 11th of June 2016.

In the game there are 2 opposing teams consisting of 5 players each. Players choose a character they will play with before game starts. As for now there are around 90 champions (character) to choose from. Each character is different and may be used for different purposes depending on their skills and abillities. In that way one can distinguish mages, assasins, fighters, supports, tanks and marksmen. Main task is to destroy enemies' defence towers resulting in concquering their base (yup, it's very boring but somehow very popular).

The game was getting more and more attention in Poland for a couple of years now. The graph below presents interest over time for google query "Mobile Legends" and "MOBA" in Poland. As you can see around 2017 there was a hugh increase in popularity of Mobile Legends while interest in MOBA games in general was falling down gradually in past 5 years. However in March and April 2020 they experienced a rapid renaissance. We can probably associate it at least in part with a lockdown caused by COVID-19 outbreak.


```{r, echo = FALSE, message = FALSE}
results <- gtrends(c("mobile legends", "MOBA"), geo = "PL", time = "2015-01-01 2020-04-04")

res_time <- results$interest_over_time

res_plot <- ggplot(res_time,aes(x = date, y = hits, col = keyword)) +
  geom_line()+
  scale_color_manual(values=c("darkred", "steelblue"))+
  ggtitle("Interest in 'Mobile Legends' and 'MOBA' in Google (Poland)") +
  theme_classic()+
  theme(legend.position = "none")
 

ggplotly(res_plot, dynamicTicks = TRUE, height = 600, width = 850) %>%
  rangeslider() %>%
  layout(hovermode = "x")
```

As stated above there are several types of characters in that game so we will try to label them based on their characteristics. In order to do so we are going to implement Principal Component Analysis to reduce dimentionallity and then a hierarchical algorithm to cluster them. Although the labels are known as such this analysis may be helpful for maintaining characters' skillsets in a balance way.

## Data 

First we have to collect the data. As there is no official site with the data on champions characteristics we will scrape it from [mobile league wiki site](https://mobile-legends.fandom.com/wiki/Mobile_Legends_Wiki). Let's check *robot.txt* file before we start.

```{r, warning=F, eval = FALSE, echo = TRUE}
paths_allowed("https://mobile-legends.fandom.com/wiki/Mobile_Legends_Wiki")
```

The upper command returns value `TRUE`. That's nice - we are allowed to scrape their data. For that purpose we will combine `rvest` package and [selector gadget widget](https://selectorgadget.com/). Whole scraping/wrangling code is provided in a speparate Rmd file in gitHub repository.

Let's have a look on how our data looks like. In the table below you can find all characters in alphabetical order. 

One important remark is although the list below present all playable characters right now we will consider it sample since the characters set is being constantly updated with new characters - in that way statistical inference can be justified.

```{r, echo = FALSE}
load("scraped_skills_data.Rda")
  
var.labels = c(Id = "Id", HERO="Hero", MV_SPD="Movement speed",
               MGC_DFN = "Magic Resistance", MANA = "Mana",
               HP_RGN = "HP Regen Rate", P_ATK = "Physical Attack",
               P_DFN = "Armor", HP = "Health points", 
               ATK_SPD = "Attack speed", MANA_RGN = "Mana regen rate",
               ROLE = "Role", SPECIALLY = "Special")

Hmisc::label(df) = as.list(var.labels[match(names(df), names(var.labels))])


kable(df, col.names = Hmisc::label(df)) %>% 
  kable_styling(fixed_thead = T, font_size = 10.75) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"))
```

One important thing we should be interested in is the variability of champions characteristics becasue if there is no variability at all or just a little even the most sophisticated analysis would be redundant. Below you can see the coefficient of variation (in %).

```{r, echo = F}
cv <- function(x){
  round((sd(x, na.rm = TRUE)/mean(x, na.rm  = TRUE))*100,2)
}

df %>%
  mutate(Id = as.character(Id))%>%
  summarize_if(is.numeric, cv) %>%
  kable(col.names = Hmisc::label(df)[-c(1,2,12,13)]) %>%
  kable_styling(font_size = 12)
```

The variabiliy of *mana* and *mana regeneration* exeed 60% - that is exactly what we were looking for! *Health points regeneration* and *armor* vary for about 16% and 26% responsively - not that much but also fine. Although the coefficient for magic resistance is at the level of 16% the value of that abillity is constant almost for every character so anyway we will drop that variable in further analysis. In any case We will have to check the data for outliers as some of those values might be inflated for instance just by a single or two observations. Rest of the variables vary just a bit (most of them under 10%). 

Now let's look for some possible relationships and check distributions of the variables.

```{r, message= F, warning=F, echo = F}
df %<>% mutate(HP = HP/1000)
  

p <- ggpairs(data=df, columns=3:11, 
             lower = list(continuous = wrap("points",alpha = 0.5, size=0.5, col = "steelblue")),
  diag = list(continuous = wrap("densityDiag", fill = "darkred", alpha = 0.8)))+
  theme_classic()

ggplotly(p, height = 800, width = 900)
```

We can see some relationships - f.e. *mana* vs. *mana regeneration* and *health points* vs. *armor* and many more - we will investigate them soon. 

Density functions for variables *movement speed*, *mana* and *mana regenerations* seem to be bimodal - it is a clear sign there are some subpopulations in our "sample" so it is reasonable to conduct cluster analysis.

There are some outliers - note a champion whose *health point regeneration* ability is about 2 times more powerful than the mean for the sample. We can also see a champion whose *health points* ability and *attack points* are extremly high. For the sake of analysis we will remove both of them from our "sample" so that they will not affect clustering results in a significant way. Let's find out who are those people.

```{r, echo = FALSE }
df[c(which.max(df$HP_RGN), which.max(df$HP)), ] %>%
  kable(col.names = Hmisc::label(df)) %>%
  kable_styling(font_size = 11)
```

The last thing we can do is to check the correlations and their significance - just to have general view since [Simson paradox](https://en.wikipedia.org/wiki/Simpson%27s_paradox) might be present. 

```{r, echo = FALSE, out.width= "65%", out.extra='style="float:right; padding:10px"'}
df_num <- df[,-c(1,2,4,12)] %>%
  as.data.frame()

rownames(df_num) <- df$HERO

df_num<- df_num[-c(which.max(df$HP_RGN), which.max(df$HP)), ] %>%
  drop_na()

correlations <- cor(df_num)
p_mat <- cor_pmat(df_num)

ggcorrplot(correlations, hc.order = TRUE, type = "lower", lab = TRUE, digits = 1,
           p.mat = p_mat, ggtheme = ggplot2::theme_classic, sig.level = 0.001,
           colors = c("steelblue", "white", "darkred"),
           title = "Pearson's correlation matrix", legend.title = latex2exp :: TeX("$\\rho$ value"),)
```


## Principal Compontent Analysis

Dealing with high dimentional data might be challenging and can lead to several problems. However in most cases it is possible to reduce the number of dimentions retaining most of the information stored in the data. One of the most widely used method that can allow us to do so is Principal Component Analysis. So what we basically want to do is to project our data matrix on some reduced-feature space using a linear transformation while restoring as much information as possible. And that is exactly what PCA does!

### How does the math look like?

Let's assume we have data matrix $X$ consisting of $n$ variables and $m$ observations, so $X \in \mathbb{R}^{n \times m}$. We want to find a linear transformation $U$ that transforms $X$ as follows: $$Z = UX, \text{ where } Z \in \mathbb{R}^{d \times m}, U \in \mathbb{R}^{d \times n} \text{ and } d<m.$$ At the same time we want make sure we mimnimize the information loss. We can think of variance-covariance matrix as a representation of information in our data. In terms of our transformed data matrix it can be denoted as $$\Sigma = \frac{1}{N}Z^TZ, \text{ where } \Sigma \in \mathbb{R}^{n\times n}.$$ Keeping that in mind searching for our transformation becomes following optimisation problem: $$\max_{U}\Sigma=\max_U\frac{1}{N}(XU)^T(XU) = \max_U\frac{1}{N}U^TX^TXU=\max_UU^T\Sigma U, \text{ where } U^TU = I.$$Note that we have to add normalization condition to make sure all of the vectors have unit magnitude because otherwise we would not be able to solve this expression as there is no upper boundary. One possible way to solve such problems is **Method of Lagrange Multipliers**.

Firstly we construct our Lagrange multiplier as following: $$F(U,\lambda)=U^T\Sigma U + \lambda(I-U^TU).$$ 

Then we differentiate it with respect to $U$ and equate to 0 as the differential should equal 0 in extremum $$\frac{dF}{dU}=\Sigma U-\lambda U.$$ 

We can rewrite it as $$\Sigma U=\lambda U.$$ 

The later looks indeed as eigenvectors equation so what we do is perform variance-covariance matrix diagonalization (eigen-decopostion) to obtain eigenvectors and corresponding eigenvalues $$\Sigma = U \Lambda U^{-1}.$$

Then we can sort pairs of eigenvectors with their eigenvalues in descending order and choose top m pairs. In that way we come up with set of m eigenvectors that retain as much part of variance as following ratio: $$\frac{\Sigma_i^m \lambda_i}{\Sigma_i \lambda_i}.$$.

Our U transformation that we are looking for is composed of the chosen eigenvectors $$U = [u_1, ..., u_m].$$


### Back to our analysis

First let's detrmine relevant prinipal components using standarized data. As scree plot would not tell us much, we should probably choose the number of compontents based on eigenvalue rule of thumb. Each of three top components has eigenvalue bigger than 1, i.e. "contains more information than a single variable". 

```{r, echo = FALSE, warning = FALSE}
pca <- prcomp(df_num, scale = TRUE)

eig.tab <- data.frame(rownames(get_eigenvalue(pca)), round(get_eigenvalue(pca),2))

colnames(eig.tab) <- c("PCA", "Eigenvalue", "Variance percent", "Cumulative variance percent")

rownames(eig.tab) <- paste0("PC",1:8)
  
eig.tab[,-1] %>%
  kable() %>%
  kable_styling() %>%
  row_spec(1:3, bold = T, color = "white", background = "darkred")
```

As you can see in the table above they account for about 70,1% of data variability. That is not as much as we expected but it's fine. We droped 5 from 8 variables and still managed to retain over 70% of variance.

Let's have a look now on the PCA loadings so we can think of some resonable interpretations.

```{r, echo = FALSE}
pca.lo <- data.frame(rownames(pca$rotation), round(pca$rotation,2))
colnames(pca.lo)[1] <- "Variable"

pca.lo%>%
  mutate_if(is.numeric, function(x) {
    cell_spec(x, bold = T, 
              color = spec_color(x, end = 0.5),
              font_size = spec_font_size(abs(x)))
  })%>%
  mutate(Variable = cell_spec(
    Variable, color = "white", bold = T,
    background = spec_color(1:8, end = 0.9, option = "C", direction = -1)
  )) %>%
  kable(escape = F, align = "c", caption = "Original loadings") %>%
  kable_styling(c("striped", "condensed"), full_width = F, position = "float_right")



ncomp<-3

rawLoadings     <- pca$rotation[,1:ncomp] %*% diag(pca$sdev, ncomp, ncomp)
rotatedLoadings <- varimax(rawLoadings)$loadings
invLoadings     <- t(pracma::pinv(rotatedLoadings))
scores          <- scale(df_num) %*% invLoadings
colnames(scores) <- c("PC1", "PC2", "PC3")

pca.rot <- rotatedLoadings[1:8,1:3]


pca.lo.rot <- data.frame(rownames(pca.rot), round(pca.rot,2))
colnames(pca.lo.rot)[1:4] <- c("Variable", "PC1", "PC2", "PC3")

pca.lo.rot%>%
  mutate_if(is.numeric, function(x) {
    cell_spec(x, bold = T, 
              color = spec_color(x, end = 0.5),
              font_size = spec_font_size(abs(x)))
  })%>%
  mutate(Variable = cell_spec(
    Variable, color = "white", bold = T,
    background = spec_color(1:8, end = 0.9, option = "C", direction = -1)
  )) %>%
  kable(escape = F, align = "c", caption = "Rotated loadings") %>%
  kable_styling(c("striped", "condensed"), full_width = F, position = "float_right")
```

As it is hard to interpret the Principal Components in that framework we may want to rotate the whole system to obtain more intuitive interpretations. For that purpose we take 3 top Principal Compontent and use orthogonal **VARIMAX** roation. We do not change cooridanate system - we roate the orthogonal basis to allign with those coordinates. In that way we assure that squared correlation between variables and factors will be maximized. On the right you can see two tables with loadings before and after rotation responisvely.

The most obvious interpretation has definitely PC2. The loading on `mana` and `mana regeneration` are very high so the underlying force here is `magic`.

PC1 has relatively high loadings (in absolute values) on `health points regeneration` and `armor` so we would lean towards some kind of `durability` interpretation.

PC3 is driven mostly by `attack points` and `attack speed` so one can interpret it as `readiness to fight`.

Now let's check whether it is possible to distinguish some clusters just by looking at the rotaed score plots. We can clearly see two clusters or maybe three... Next section will help us to understand what is going on.

```{r, echo = FALSE, warning= FALSE, message = FALSE}
q <- ggpairs(data=as.data.frame(scores), columns=1:3, 
             lower = list(continuous = wrap("points",alpha = 0.5, size=0.5, col = "steelblue")),
  diag = list(continuous = wrap("densityDiag", fill = "darkred", alpha = 0.8)))+
  theme_classic()

ggplotly(q, height = 600, width = 800)
```

## Clusters

Now as we reduced dimentionality we can proceed to the most exciting part of our analysis - clusters distinguishment. In that part we will implement hierarchical algorithm. 

First let's start with computing distances between observation. For that purpose we will use second order of Minkowski metrics, i.e. Euclidean distance. Below you can see vizualisation of discussed distance matrix.

```{r, echo =FALSE, fig.align= "center", fig.width= 10}

distance.e <- get_dist(scores, method = "euclidean")

dist.e.plot <- fviz_dist(distance.e, gradient = list(low = "steelblue", mid = "white", high = "darkred"), lab_size = 4)

plot(dist.e.plot)
```

Now we ...
UNDER 
CONSTRUCTION
KEEP
WAITNING


```{r, echo =FALSE, fig.align= "center", fig.width= 10}
hc <- hclust(distance.e, method = "ward.D2")

dend <- scores %>% dist("euclidean") %>% 
   hclust %>% as.dendrogram %>%  set("branches_k_color", k=5) %>% set("branches_lwd", 1.2) %>%
   set("labels_colors") %>% set("labels_cex", c(.6,.7)) %>% 
   set("leaves_pch", 19) %>% set("leaves_col", c("blue", "red"))

plot(dend)
```


```{r}
fviz_nbclust(scores, FUN = hcut, method = "silhouette", hc_method = "ward.D2")
fviz_nbclust(scores, FUN = hcut, method = "silhouette", hc_method = "single")
fviz_nbclust(scores, FUN = hcut, method = "silhouette", hc_method = "complete")
fviz_nbclust(scores, FUN = hcut, method = "silhouette", hc_method = "average")
```



```{r, echo =FALSE, fig.align= "center", fig.width= 10}
ggd1 <- as.ggdend(dend)

ggplot(ggd1, labels = F) + 
  scale_y_reverse(expand = c(0.2, 0)) +
  coord_polar(theta="x")
```

```{r, warning=F, message = F, echo = F}
CLUSTER <- as.factor(cutree(dend, 7))


df_class <- cbind(df_num,CLUSTER)

s <- ggpairs(data=df_class, columns=2:9, mapping = aes(color = CLUSTER),
             lower = list(continuous = wrap("points",alpha = 0.5, size=0.5)),
  diag = list(continuous = wrap("densityDiag", alpha = 0.8)))+
  theme_classic()

ggplotly(s, height = 1400, width = 1000)
```

